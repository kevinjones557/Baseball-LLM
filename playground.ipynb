{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE = 1\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        #self.mask = torch.tril(torch.ones(config.context_length, config.context_length)).view(1, 1, config.context_length, config.context_length).to(config.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimension\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # batch size, num heads, sequence length, embedding size\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # att = (q @ k.transpose(-2, -1)) * 1.0 / math.sqrt(k.size(-1))\n",
    "        # att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.SCALE = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.c_proj(self.gelu(self.c_fc(x)))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    context_length: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    device: str = 'cuda'\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.context_length, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight   \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, \"SCALE\"):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.context_length\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device = idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "         \n",
    "        x - self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # B, T, vocab_size\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params' : decay_params, 'weight_decay' : weight_decay},\n",
    "            {'params' : nodecay_params, 'weight_decay' : 0.0}\n",
    "        ]\n",
    "\n",
    "        return torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        with open(\"shakespeare.txt\", 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        self.curent_pos = 0\n",
    "\n",
    "        print(f\"1 epoch is {len(tokens) // (B * T)} batches\")\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.curent_pos:self.curent_pos + B*T + 1]\n",
    "        x = buf[:-1].view(B, T)\n",
    "        y = buf[1:].view(B, T)\n",
    "\n",
    "        self.curent_pos += B*T\n",
    "        if self.curent_pos + (B * T + 1) > len(self.tokens):\n",
    "            self.curent_pos = 0\n",
    "        return x,  y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = GPT(GPTConfig(vocab_size=50304)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "max_lr = 12e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "val_check_freq = 50\n",
    "\n",
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    decay_ration = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ration <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ration))\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "total_batch_size = 2 ** 19\n",
    "B = 4\n",
    "T = 1024\n",
    "assert total_batch_size % (B * T) == 0\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "print(f\"grad accum steps = {grad_accum_steps}\")\n",
    "loader = DataLoaderLite(B, T)\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4)\n",
    "for step in range(max_steps):\n",
    "    t1 = time.time()\n",
    "    loss_accum = 0.0\n",
    "    if step % val_check_freq == 0 or step == max_steps - 1:\n",
    "        checkpoint = {\n",
    "            'model' : model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'config' : model.config,\n",
    "            'step' : step,\n",
    "        }\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss /= grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    norm = nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t2 = time.time()\n",
    "    dt = (t2 - t1) * 1000\n",
    "    tok_per_sec = (loader.B * loader.T * grad_accum_steps) / (t2 - t1)\n",
    "    print(f\"step {step} | loss: {loss_accum} | norm: {norm:.4f} | lr: {lr:.4e} | time: {dt} | tokens / second: {tok_per_sec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = tiktoken.get_encoding('gpt2')\n",
    "# tokens = enc.encode(\"A strike is a\")\n",
    "# x = torch.tensor(tokens, dtype=torch.long).to(device)\n",
    "# x = x.unsqueeze(0)\n",
    "# max_length = 30\n",
    "# while x.size(1) < max_length:\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(x)\n",
    "#         logits = logits[:, -1, :]\n",
    "#         probs = F.softmax(logits, dim=-1)\n",
    "#         topk_probs, topk_indicies = torch.topk(probs, 50, dim=-1)\n",
    "#         ix = torch.multinomial(topk_probs, 1)\n",
    "#         xcol = torch.gather(topk_indicies, -1, ix)\n",
    "#         x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# tokens = x[-1].tolist()\n",
    "# print(enc.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from model.gpt import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "checkpoint = torch.load('model/finetuned.pth')\n",
    "device = \"cuda\"\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304)).to(device)\n",
    "\n",
    "# Restore the model state\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "start = \"Who created you?\"\n",
    "tokens = enc.encode(start)\n",
    "print(start)\n",
    "x = torch.tensor(tokens, dtype=torch.long).to(device)\n",
    "x = x.unsqueeze(0)\n",
    "max_length = len(start) + 10000\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(x)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_indicies = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        xcol = torch.gather(topk_indicies, -1, ix)\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "        next_char = xcol.squeeze(0).tolist()\n",
    "        if xcol.squeeze(0).tolist() == [50256]:\n",
    "            break\n",
    "        print(enc.decode(next_char), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "dataset = load_dataset(\"berkeley-nest/Nectar\")\n",
    "\n",
    "qa_pairs = []\n",
    "\n",
    "for entry in dataset['train']:\n",
    "    question = entry['prompt']\n",
    "    question = question.split('Human:')[1].split('\\n\\nAssistant:')[0].strip()\n",
    "    answers = entry['answers']\n",
    "    for answer in answers:\n",
    "        ans = answer['answer']\n",
    "        if ans != \"\"\"I'm sorry, but I can't assist with that.\"\"\":\n",
    "            tokens = enc.encode(question + ' ' + ans + '<|endoftext|>', allowed_special={'<|endoftext|>'})\n",
    "            if len(tokens) <= 1024:\n",
    "                qa_pairs.append(tokens)\n",
    "\n",
    "with open('qa_pairs.pkl', 'wb') as f:\n",
    "    pickle.dump(qa_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "with open('qa_pairs.pkl', 'rb') as f:\n",
    "    pairs = pickle.load(f)\n",
    "random.shuffle(pairs)\n",
    "\n",
    "with open('qa_pairs.pkl', 'wb') as f:\n",
    "    pickle.dump(pairs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pairs) * .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# new_pairs = []\n",
    "# for pair in tqdm(pairs):\n",
    "#     raw_text = pair[0] + ' ' + pair[1]\n",
    "#     tokens = enc.encode(raw_text, allowed_special={'<|endoftext|>'})\n",
    "#     if len(tokens) <= 1024:\n",
    "#         new_pairs.append(tokens)\n",
    "\n",
    "# with open('qa_pairs.pkl', 'wb') as f:\n",
    "#     pickle.dump(new_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "with open('qa_pairs.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import tiktoken \n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "dataset = load_dataset(\"caball21/baseball\")\n",
    "\n",
    "qa_pairs = []\n",
    "\n",
    "for entry in dataset['train']:\n",
    "    question = entry['input']\n",
    "    instruct = entry['instruction']\n",
    "    answer = entry['output']\n",
    "    if answer is None or question is None or instruct is None:\n",
    "        continue\n",
    "    combined = question + ' ' + instruct + ' ' + answer + \"<|endoftext|>\"\n",
    "    qa_pairs.append(enc.encode(combined, allowed_special={\"<|endoftext|>\"}))\n",
    "\n",
    "with open('baseball_pairs.pkl', 'wb') as f:\n",
    "    pickle.dump(qa_pairs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with open(\"validation_loss_log.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for line in data:\n",
    "    x.append(int(line.split()[0]))\n",
    "    y.append(float(line.split()[-1]))\n",
    "\n",
    "plt.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"validation_finetuning_loss_log.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for line in data:\n",
    "    x.append(int(line.split()[3]))\n",
    "    y.append(float(line.split()[-1]))\n",
    "\n",
    "plt.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/baseball data/baseball_pairs.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/baseball data/baseball_pairs.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "question = \"\"\n",
    "res = []\n",
    "for l in lines:\n",
    "    if l == '\\n':\n",
    "        continue\n",
    "    if l[0] == 'Q':\n",
    "        question = l[3:]\n",
    "    elif l[0] == 'A':\n",
    "        text = question + ' ' + l[3:] + '<|endoftext|>'\n",
    "        text = text.replace('\\n', '')\n",
    "        res.append(enc.encode(text, allowed_special={'<|endoftext|>'}))\n",
    "\n",
    "with open('data/baseball data/baseball_qa.pkl', 'wb') as f:\n",
    "    pickle.dump(res, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 6, 69, 0]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
